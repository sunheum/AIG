{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "\n",
    "# Mysql DB 접속\n",
    "iscream_questions = pymysql.connect(\n",
    "    user='root', \n",
    "    passwd='', \n",
    "    host='192.168.1.30', \n",
    "    db='iscream_questions', \n",
    "    charset='utf8'\n",
    ")\n",
    "\n",
    "cursor = iscream_questions.cursor(pymysql.cursors.DictCursor)\n",
    "\n",
    "sql = \"SELECT question_id, body_html, list_html, explanation_html FROM questions_78786\\\n",
    "        UNION\\\n",
    "        SELECT question_id, body_html, list_html, explanation_html FROM questions_120562 WHERE etc_category1 LIKE '%중학교 1학년%'\\\n",
    "        UNION\\\n",
    "        SELECT question_id, body_html, list_html, explanation_html FROM questions_120562 WHERE etc_category1 LIKE '%중학교 2학년%'\\\n",
    "        UNION\\\n",
    "        SELECT question_id, body_html, list_html, explanation_html FROM questions_120562 WHERE etc_category1 LIKE '%중학교 3학년%'\\\n",
    "        UNION\\\n",
    "        SELECT question_id, body_html, list_html, explanation_html FROM questions_120562 WHERE etc_category1='이관' AND body_title_html NOT regexp('①|더미')\"\n",
    "\n",
    "cursor.execute(sql)\n",
    "mysql_result = cursor.fetchall()\n",
    "\n",
    "question_id_list = []\n",
    "body_html_list = []\n",
    "list_html_list = []\n",
    "explanation_html_list = []\n",
    "\n",
    "for i in mysql_result:\n",
    "    question_id_list.append(i['question_id'])\n",
    "    body_html_list.append(i['body_html'])\n",
    "    list_html_list.append(i['list_html'])\n",
    "    explanation_html_list.append(i['explanation_html'])\n",
    "    \n",
    "# print(question_id_list)\n",
    "# print(body_html_list)\n",
    "# print(list_html_list)\n",
    "# print(explanation_html_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html 태그 제거\n",
    "def removetag(list_name):\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    list_modify = []\n",
    "    for i in range(len(list_name)):\n",
    "        list_name[i]=re.sub('<.+?>', '', list_name[i], 0).strip()\n",
    "        list_name[i] = list_name[i].replace(\"$$\", \"\")\n",
    "        list_name[i] = list_name[i].replace(\"\\\\hspace{1.2pt}\", \"\")\n",
    "        list_name[i] = list_name[i].replace(\"&nbsp;\", \"\")\n",
    "        list_name[i] = list_name[i].replace(\"[#NO#]\", \"\")\n",
    "        list_name[i] = list_name[i].replace(\"\\n\", \"\")\n",
    "        list_name[i] = list_name[i].replace(\"\\t\", \"\")\n",
    "        list_name[i] = list_name[i].replace(\"&lt;\", \" \")\n",
    "        list_name[i] = list_name[i].replace(\"&gt;\", \" \")\n",
    "        list_name[i] = list_name[i].replace(\"•\", \"\")\n",
    "        list_name[i] = list_name[i].replace(\"\\r\", \"\")\n",
    "        list_name[i] = list_name[i].replace(\"　\", \"\") # \\u3000 제거\n",
    "        list_name[i] = list_name[i].replace(\"문제시작/\", \"\")\n",
    "        list_name[i] = list_name[i].replace(\"해설시작/\", \"\")\n",
    "#         list_name[i] = list_name[i].replace(\"■▣\", \"\")\n",
    "#         list_name[i] = list_name[i].replace(\"\", \"\")\n",
    "        list_name[i] = list_name[i].strip()\n",
    "        \n",
    "        # 배점, 채점기준 내용 제거\n",
    "        a = re.search(\"배점\", list_name[i])\n",
    "        if a!=None:\n",
    "            list_name[i] = list_name[i].split(\"배점\")[0]\n",
    "        \n",
    "        list_modify.append(list_name[i])\n",
    "    \n",
    "    return list_modify\n",
    "\n",
    "body_html_list_modify = removetag(body_html_list)\n",
    "list_html_list_modify = removetag(list_html_list)\n",
    "explanation_html_list_modify = removetag(explanation_html_list)\n",
    "\n",
    "# body_html_list_modify\n",
    "# list_html_list_modify\n",
    "# explanation_html_list_modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103414\n"
     ]
    }
   ],
   "source": [
    "total_list = []\n",
    "\n",
    "for i in range(len(body_html_list_modify)):\n",
    "    tot = body_html_list_modify[i] + \" \" + list_html_list_modify[i] + \" \" + explanation_html_list_modify[i]\n",
    "    if '  ' != tot:\n",
    "        total_list.append(tot)\n",
    "\n",
    "print(len(total_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createtxt(list_name):\n",
    "    \n",
    "    import datetime\n",
    "    \n",
    "    basename = \"createtxt\"\n",
    "    suffix = datetime.datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
    "    filename = \"_\".join([basename, suffix])\n",
    "    \n",
    "    f = open('{}.txt'.format(filename), 'w', encoding='utf-8')\n",
    "    \n",
    "    for i in range(len(list_name)):\n",
    "        f.write('%s\\n' %list_name[i])\n",
    "    f.close()\n",
    "\n",
    "createtxt(total_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordExtractor 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 2.211 Gbse memory 2.278 Gb\n",
      "all cohesion probabilities was computed. # words = 37446\n",
      "all branching entropies was computed # words = 219148\n",
      "all accessor variety was computed # words = 219148\n"
     ]
    }
   ],
   "source": [
    "from soynlp.word import WordExtractor\n",
    "# from soynlp.utils import DoublespaceLineCorpus\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "\n",
    "# file_path = 'your file path'\n",
    "# corpus = DoublespaceLineCorpus(file_path, iter_sent=True)\n",
    "\n",
    "word_extractor = WordExtractor(min_frequency=20,\n",
    "    min_cohesion_forward=0.05, \n",
    "    min_right_branching_entropy=0.0\n",
    ")\n",
    "word_extractor.train(total_list)\n",
    "words = word_extractor.extract()\n",
    "\n",
    "## 단어 점수 중 Forward cohesion 의 점수만을 이용\n",
    "cohesion_score = {word:score.cohesion_forward for word, score in words.items()}\n",
    "tokenizer = LTokenizer(scores=cohesion_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어   (빈도수, cohesion, branching entropy)\n",
      "\n",
      "입니다.     (12396, 0.905, 4.674)\n",
      "따라서     (23831, 0.951, 4.484)\n",
      "합니다.     (8167, 0.669, 4.670)\n",
      "있습니다.     (9096, 0.678, 4.637)\n",
      "했습니다.     (400, 0.814, 4.447)\n",
      "였습니다.     (21, 0.801, 4.455)\n",
      "에서     (16731, 0.989, 4.014)\n",
      "으로     (1850, 0.998, 3.931)\n",
      "됩니다.     (2308, 0.971, 3.883)\n",
      "구하시오.     (8732, 0.738, 4.131)\n",
      "많습니다.     (899, 0.574, 4.349)\n",
      ")입니다.     (166, 0.520, 4.438)\n",
      "누구입니까?     (427, 0.754, 4.066)\n",
      "까지     (1141, 0.971, 3.787)\n",
      "한다.     (5615, 0.891, 3.823)\n",
      ".따라서     (30, 0.462, 4.449)\n",
      "cm     (13843, 0.965, 3.699)\n",
      "필요한     (1644, 0.729, 3.964)\n",
      "동안     (4399, 0.372, 4.632)\n",
      "것입니다.     (2138, 0.481, 4.347)\n",
      "갔습니다.     (186, 0.817, 3.812)\n",
      "보세요.     (10211, 0.705, 3.959)\n",
      "큽니다.     (929, 0.978, 3.628)\n",
      "필요합니다.     (491, 0.692, 3.972)\n",
      ")따라서     (136, 0.391, 4.506)\n",
      "m입니다.     (238, 0.441, 4.343)\n",
      "쓰시오.     (6077, 0.811, 3.730)\n",
      "cm입니다.     (960, 0.582, 4.049)\n",
      "개입니까?개     (1304, 0.496, 4.201)\n",
      "개입니다.     (1644, 0.441, 4.315)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def word_score(score):\n",
    "    return (score.cohesion_forward * math.exp(score.right_branching_entropy))\n",
    "\n",
    "print('단어   (빈도수, cohesion, branching entropy)\\n')\n",
    "for word, score in sorted(words.items(), key=lambda x:word_score(x[1]), reverse=True)[:30]:\n",
    "    print('%s     (%d, %.3f, %.3f)' % (\n",
    "            word, \n",
    "            score.leftside_frequency, \n",
    "            score.cohesion_forward,\n",
    "            score.right_branching_entropy\n",
    "            )\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['도형', '가는', '정사각형', '이고,', '도형', '나는', '직사각형', '입니다.', '두', '도형', '중', '네', '변의', '길이', '의', '합이', '더', '긴', '것을', '찾아', '기호를', '쓰시오.', '가', ':', '13+1', '3+13+13=52(cm)나', ':', '6+9+6+9=30(cm)따라서', '네', '변의', '길이', '의', '합이', '더', '긴', '도형', '은', '가입니다.']\n"
     ]
    }
   ],
   "source": [
    "sent = \"{}\".format(total_list[29375])\n",
    "\n",
    "print(tokenizer.tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Scores(cohesion_forward=0.5045571557781953, cohesion_backward=0.015538033395176253, left_branching_entropy=3.2844340860587, right_branching_entropy=2.246065650381712, left_accessor_variety=109, right_accessor_variety=57, leftside_frequency=7252, rightside_frequency=201)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words['도형']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LRNounExtractor_2 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 183343 from 103414 sents. mem=2.034 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=9305121, mem=2.106 Gb\n",
      "[Noun Extractor] batch prediction was completed for 42796 words\n",
      "[Noun Extractor] checked compounds. discovered 44314 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 53936 -> 47411\n",
      "[Noun Extractor] postprocessing ignore_features : 47411 -> 47328\n",
      "[Noun Extractor] postprocessing ignore_NJ : 47328 -> 47271\n",
      "[Noun Extractor] 47271 nouns (44314 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=2.186 Gb                    \n",
      "[Noun Extractor] 53.08 % eojeols are covered\n"
     ]
    }
   ],
   "source": [
    "from soynlp.noun import LRNounExtractor_v2\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "\n",
    "noun_extractor = LRNounExtractor_v2()\n",
    "nouns = noun_extractor.train_extract(total_list)\n",
    "\n",
    "## Cohesion 점수 + 명사 점수 사용\n",
    "# noun_scores = {noun:score.score for noun, score in nouns.items()}\n",
    "# combined_scores = {noun:score + cohesion_score.get(noun, 0)\n",
    "#     for noun, score in noun_scores.items()}\n",
    "# combined_scores.update(\n",
    "#     {subword:cohesion for subword, cohesion in cohesion_score.items()\n",
    "#     if not (subword in combined_scores)}\n",
    "# )\n",
    "\n",
    "# tokenizer = LTokenizer(scores=combined_scores)\n",
    "\n",
    "## 명사 점수 사용\n",
    "noun_scores = {noun:score.score for noun, score in nouns.items()}\n",
    "tokenizer = LTokenizer(scores=noun_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NounScore(frequency=302, score=1.0)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns['중근']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['어떤', '수를', '3배', '하여', '7을', '뺀', '수는', '어떤', '수의', '2배', '보다', '5만큼', '크다.', '어떤', '수는', '?']\n"
     ]
    }
   ],
   "source": [
    "sent = \"어떤 수를 3배 하여 7을 뺀 수는 어떤 수의 2배보다 5만큼 크다. 어떤 수는?\"\n",
    "print(tokenizer.tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaxScoreTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['다음', '일차', '방정식의', '해의', '개수는']\n"
     ]
    }
   ],
   "source": [
    "from soynlp.tokenizer import MaxScoreTokenizer\n",
    "\n",
    "scores = {'보다 크고': 1.0, '일차방정식':0.5, '일차 방정식':0.5}\n",
    "tokenizer = MaxScoreTokenizer(scores=scores)\n",
    "\n",
    "print(tokenizer.tokenize('다음 일차 방정식의 해의 개수는'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 명사만으로 문제 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103414\n"
     ]
    }
   ],
   "source": [
    "tokenized_list = []\n",
    "for i in range(len(total_list)):\n",
    "    temp = []\n",
    "    temp2 = []\n",
    "    temp.append(tokenizer.tokenize(total_list[i]))\n",
    "    for j in range(len(temp[0])):\n",
    "        if temp[0][j] in nouns:\n",
    "            temp2.append(temp[0][j])\n",
    "    tokenized_list.append(temp2)\n",
    "\n",
    "print(len(tokenized_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "result = tokenized_list\n",
    "\n",
    "model = Word2Vec(result, \n",
    "                 size=100,         # 차원\n",
    "                 window=5,         # 앞뒤 고려 단어수\n",
    "                 min_count=10,     # 최소 중복 개수\n",
    "                 workers=4,        # CPU 코어\n",
    "                 sg=1,             # CBOW / Skip-Gram\n",
    "                 iter = 5,         # iteration\n",
    "                 sample = 0.001)   # 빈번하게 등장하는 단어 다운샘플링\n",
    "\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('이차방정식', 0.8205578327178955),\n",
       " ('완전제곱식', 0.7952829003334045),\n",
       " ('판별식', 0.7654517889022827),\n",
       " ('(중근)', 0.7142831087112427),\n",
       " ('공통인수', 0.7140236496925354)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=\"중근\", topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습된 토크나이저를 이용하여 문서를 희소행렬로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanning was done                                        \n",
      "121023 terms are recognized\n",
      "transforming docs to term frequency marix was done\n"
     ]
    }
   ],
   "source": [
    "from soynlp.vectorizer import BaseVectorizer\n",
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "\n",
    "vectorizer = BaseVectorizer(\n",
    "    tokenizer=tokenizer,\n",
    "    min_tf=0,\n",
    "    max_tf=10000,\n",
    "    min_df=0,\n",
    "    max_df=1.0,\n",
    "    stopwords=None,\n",
    "    lowercase=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "file_path = './questions/total_list.txt'\n",
    "corpus = DoublespaceLineCorpus(file_path, iter_sent=True)\n",
    "\n",
    "corpus.iter_sent = False\n",
    "x = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<103429x121023 sparse matrix of type '<class 'numpy.intc'>'\n",
       "\twith 2614378 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[353, 718, 14620]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.encode_a_doc_to_list('다음 일차방정식의 해를 구하시오')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['일차방정식', '해를', '구하시오']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.decode_from_list([353, 718, 14620])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('문자', 'Noun'), ('를', None)]\n"
     ]
    }
   ],
   "source": [
    "pos_dict = {\n",
    "#     'Adverb': {'모두'}, \n",
    "    'Noun': {'문자', '일차방정식', '상수'},\n",
    "#     'Josa': {'을'},\n",
    "#     'Verb': {'고르면'},\n",
    "#     'Adjective': {'예쁜', '예쁘다'},\n",
    "#     'Exclamation': {'우와'}    \n",
    "}\n",
    "\n",
    "from soynlp.postagger import Dictionary\n",
    "from soynlp.postagger import LRTemplateMatcher\n",
    "from soynlp.postagger import LREvaluator\n",
    "from soynlp.postagger import SimpleTagger\n",
    "from soynlp.postagger import UnknowLRPostprocessor\n",
    "\n",
    "dictionary = Dictionary(pos_dict)\n",
    "generator = LRTemplateMatcher(dictionary)    \n",
    "evaluator = LREvaluator()\n",
    "postprocessor = UnknowLRPostprocessor()\n",
    "tagger = SimpleTagger(generator, evaluator, postprocessor)\n",
    "\n",
    "sent = '다음 중 문자를 사용한 식을 모두 고르면?'\n",
    "print(tagger.tag(sent))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
